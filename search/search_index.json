{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"llm-deployments/chat-models/deployment/","title":"Chat hosted models: vLLM framework","text":""},{"location":"llm-deployments/chat-models/deployment/#self-hosted-models","title":"Self hosted models","text":"<p>Self-model hosting involves deploying the LLM model within an organization's private infrastructure. This setup ensures that the model and data stay on-premises or within a secure network, which can be crucial for applications with strict data privacy or security requirements.</p>"},{"location":"llm-deployments/chat-models/deployment/#what-is-the-vllm","title":"What is the vLLM ?","text":"<p>vLLM is a library designed to easily serve LLMs for inference with a high throughput. It is meant to take care of the highly complex deployment of models and ensuring an effective use of GPU memory, thus lowering the deployment time and the hardware requirements.</p> <p>Some useful resources as a starting point are:  vLLM website vLLM Github repo Conference talk about vLLM</p>"},{"location":"llm-deployments/chat-models/deployment/#why-do-we-need-vlmm","title":"Why do we need vLMM?","text":"<p>Serving LLMs can be as sluggish even with pricey hardware. vLLM is the ultimate open-source toolkit for lightning-fast LLM inference and serving. It packs a secret weapon called PagedAttention, a brainy algorithm that handles LLM\u2019s attention keys and values like a champ. LLM-serving tool delivers up to 24x more speed than HuggingFace (HF) Transformers and and a whopping 3.5x more speed than HuggingFace Text Generation Inference (TGI).</p> <p>vLLM offers several key features that sets it apart:</p>"},{"location":"llm-deployments/chat-models/deployment/#1-llm-deployment","title":"1. LLM Deployment","text":"<p>The deployment of LLMs can be quite challenging as it requires a lot of dependencies and technologies like CUDA, docker, various python libraries and some kind of API framework. There are tools like Huggingface's Text-Generation-Inference (TGI) , Nvidia Triton inference server, and FastAPI to make the deployment easier, but they might lead to throughput issues.</p>"},{"location":"llm-deployments/chat-models/deployment/#2-high-throughput-of-inference-requests","title":"2. High throughput of inference requests","text":"<p>vLLM outperforms the Huggingface (HF) deployment frameworks it achieves up to 24x higher throughput compared to HF and up to 3.5x higher throughput than TGI. </p>"},{"location":"llm-deployments/chat-models/deployment/#3-seamless-integration","title":"3. Seamless Integration:","text":"<p>One of vLLM's strengths is its compatibility with various HuggingFace models, including architectures like GPT-2, GPT-NeoX, Falcon, and more.</p>"},{"location":"llm-deployments/chat-models/deployment/#how-to-get-started-with-vllm-and-local-deployment","title":"How to get started with vLLM and Local Deployment","text":""},{"location":"llm-deployments/chat-models/deployment/#installation","title":"Installation:","text":"<p>Installation Guide</p> <p>Requirements: OS: Linux; Python: 3.8 \u2013 3.11; GPU: compute capability 7.0 or higher (e.g., V100, T4, RTX20xx, A100, L4, etc.)</p>"},{"location":"llm-deployments/chat-models/deployment/#option-1-install-with-pip","title":"Option 1. Install with pip:","text":"<pre><code># (Optional) Create a new conda environment.\nconda create -n myenv python=3.8 -y\nconda activate myenv\n\n# Install vLLM.\npip install vllm\n</code></pre>"},{"location":"llm-deployments/chat-models/deployment/#option-2-build-from-source","title":"Option 2. Build from source:","text":"<p>You can also build and install vLLM from source: <pre><code>git clone https://github.com/vllm-project/vllm.git\ncd vllm\npip install -e .  # This may take 5-10 minutes.\n</code></pre></p>"},{"location":"llm-deployments/chat-models/deployment/#quickstart","title":"Quickstart:","text":"<p>Quickstart</p> <p>Before we start with the topic of Quickstart, it is worth noting the supported models and the option of adding a new model.</p>"},{"location":"llm-deployments/chat-models/deployment/#list-of-supported-models","title":"List of supported models:","text":"<p>https://vllm.readthedocs.io/en/latest/models/supported_models.html#supported-models</p>"},{"location":"llm-deployments/chat-models/deployment/#adding-a-new-model","title":"Adding a new model:","text":"<p>https://vllm.readthedocs.io/en/latest/models/adding_model.html#adding-a-new-model</p>"},{"location":"llm-deployments/chat-models/deployment/#option-1-offline-batch-inference","title":"Option 1: Offline Batch Inference:","text":"<p>Here we have an example to use vLLM as offline batched inference on a dataset. We use the OPT-125M model.</p> <p>Example offline Batch Inference: https://github.com/vllm-project/vllm/blob/main/examples/offline_inference.py</p>"},{"location":"llm-deployments/chat-models/deployment/#option-2-api-server","title":"Option 2: API Server:","text":"<p>build an API server for a large language model; Simple Example:</p>"},{"location":"llm-deployments/chat-models/deployment/#start-the-server","title":"Start the server:","text":"<p>python -m vllm.entrypoints.api_server</p> <p>Query the model in shell:</p> <pre><code>curl http://localhost:8000/generate \\\n\n    -d '{\n        \"prompt\": \"San Francisco is a\",\n        \"use_beam_search\": true,\n        \"n\": 4,\n        \"temperature\": 0\n    }'\n</code></pre> <p>For a more detailed client example: https://github.com/vllm-project/vllm/blob/main/examples/api_client.py</p>"},{"location":"llm-deployments/chat-models/deployment/#option-3-openai-compatible-server","title":"Option 3: OpenAI-Compatible Server:","text":"<p>vLLM can be deployed as a server that mimics the OpenAI API protocol. This allows vLLM to be used as a drop-in replacement for applications using OpenAI API</p> <p>Example: https://github.com/vllm-project/vllm/blob/main/examples/openai_completion_client.py</p>"},{"location":"llm-deployments/chat-models/deployment/#local-deployment","title":"Local Deployment:","text":"<pre><code>There are four options for Deployment. For us to deploy vLLM on AI Core the best option is to deploy it as a Docker\n</code></pre>"},{"location":"llm-deployments/chat-models/deployment/#deploying-with-docker","title":"Deploying with Docker:","text":"<p>https://vllm.readthedocs.io/en/latest/serving/deploying_with_docker.html You can build and run vLLM from source via the provided dockerfile. To build vLLM:</p> <pre><code>DOCKER_BUILDKIT=1 docker build . --target vllm --tag vllm --build-arg max_jobs=8\n\ndocker run --runtime nvidia --gpus all \\\n    -v ~/.cache/huggingface:/root/.cache/huggingface \\\n    -p 8000:8000 \\\n    --env \"HUGGING_FACE_HUB_TOKEN=&lt;secret&gt;\" \\\n    vllm &lt;args...&gt;\n</code></pre>"},{"location":"llm-deployments/chat-models/deployment/#vllm-deployment-on-sap-ai-core","title":"vLLM deployment on SAP AI Core","text":"<ol> <li>Creating Models on AI Launchpad (ML Operations)</li> </ol> <p>Approach: To deploy the vLLM on AI Core, there are two different approaches. The diagram below clearly shows both these approaches: </p>"},{"location":"llm-deployments/chat-models/deployment/#pre-requisites","title":"Pre-requisites","text":"<p>This guide assumes:</p> <p>1) Familiarity with SAP AI Core. See SAP AI Core on BTP.  2) Understanding the SAP AI Core deployment workflow. See SAP AI Core ML Operations Documentation. 3) Access to JFrog Artifactory. See Gain Access to SAP Internet Facing Common Repository.     3.1) For write access to push Docker images,please contact the admin person or 3) Please use docker hub for push image. 4) Possession of an active Hugging Face account. 5) Access to the model hosted on HF. 6) Access to the ADB Pipeline .</p>"},{"location":"llm-deployments/chat-models/deployment/#understanding-resources-need-for-deployment","title":"Understanding Resources Need for Deployment","text":"<p>Before deploying a LLM on AI Core using vLLM, you will first need to understand the resource plans currently available, as well as the resources required for the model.</p> <p>The available resource plans on AI Core for IES deployment workflow (tenants with sap-internal service-plan) are the following:</p> ResourcePlan ID GPUs GPU Memory Label infer2.l 1 A10G 1 x 24 GB =   24 GB ai.sap.com/resourcePlan: infer2.l infer2.4xl 4 A10G 4 x 24 GB =   96 GB ai.sap.com/resourcePlan: infer2.4xl train2.8xl 8 A100 8 x 40 GB = 320 GB ai.sap.com/resourcePlan: train2.8xl train2.8xxl 8 A100 8 x 40 GB = 320 GB ai.sap.com/resourcePlan: train2.8xxl <p>For LLM serving the most important resource KPI is the GPU memory, which depends on how much space the model parameters consume. The model parameters give a rough estimate on how much GPU memory is required. Depending on the precision used i.e. how many digit does each model weight (parameter) have. Think of it in terms of position after decimal point. It is usually 4, 8, 16, 32 bit, which are 0.5, 1, 2, 4 byte. 1 billion bytes are 1GB. As an example Llama-2-70b-chat full precision is 16bit therefore therefore inference requires at least 140 GB of GPU RAM just to load the model into the memory. At FP32, you would need double of that (about ~260Gb to load the model). Furthermore iit is important to understand, that depending on how the LLM is deployed it might only use one GPU and therefore leading to an out of memory error, despite having theoretically enough, when all GPU memory is added up. This very much depends on what is used for inference e.g. vLLM or PyTorch directly, etc. and if the docker container is configured correctly e.g. all the dependencies and PATHS are correctly installed. Also the hardware might not be ready for distributive inference as some GPUs do not support multi-GPU memory distribution. If you want to more about hardware requirements this blog article is a good start: https://www.baseten.co/blog/llm-transformer-inference-guide/ and this Databricks article goes a little more into depth https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices</p>"},{"location":"llm-deployments/chat-models/deployment/#building-vllm-inference-server-docker-image","title":"Building vLLM Inference Server Docker Image","text":"<p>As the vLLM Inference Server Docker image is fundamentally a Docker image that only contains the vLLM setup, individual developers would NOT be required to build a new vLLM image every time a new deployment is conducted. As such, a standardised repository path is created to host the [latest] image that can be re-used across different deployments.</p> <p><pre><code># From dir where you want to clone the repository:\ngit clone &lt;https://github.tools.sap/Artificial-Intelligence-CoE/AICore-vLLM.git&gt;\n\n# From the AICore-LLM directory\ncd inference-server\n\n# Build docker image\ndocker build -t &lt;dockerRepository&gt;/aicore-vllm:v&lt;ddmmyy&gt;-&lt;minorVersion&gt; .\n# e.g.,\n# docker build -t ies-aicoe.common.repositories.cloud.sap/genai-platform-exp/aicore-vllm:v090124-01 .\n\n# Push docker image to the registered repository\ndocker push &lt;dockerRepository&gt;/aicore-vllm:v&lt;ddmmyy&gt;-&lt;minorVersion&gt;\n# e.g.,\n# docker push ies-aicoe.common.repositories.cloud.sap/genai-platform-exp/aicore-vllm:v090124-01\n</code></pre> For standardisation purposes, the following naming convention denoted above must be adhered: </p> <p>1) Our container image repository is located at: ies-aicoe.common.repositories.cloud.sap/genai-platform-exp&gt;  2) vLLM images are to named as: aicore-vllm  3) Version tag format are to be named as: v-, where the minorVersion starts from 01. <p>An example of the full naming structure would be:</p> <pre><code>ies-aicoe.common.repositories.cloud.sap/genai-platform-exp/aicore-vllm:v090124-01\n</code></pre>"},{"location":"llm-deployments/chat-models/deployment/#downloading-huggingface-model-artefacts-and-saving-to-s3-object-store","title":"Downloading HuggingFace Model Artefacts and saving to S3 Object Store","text":""},{"location":"llm-deployments/chat-models/deployment/#1-using-hf-downloader-and-aws-cli","title":"1. Using HF downloader and AWS CLI","text":"<p>1.Download the model using HF downloader. <pre><code>NOTE: This section is currently demonstrating the local download process of model artefacts.\nIdeally, we will be implementing a pipeline to handle both download and transferring of model artefacts in the subsequent sprints.\n</code></pre> To download the model and its corresponding artefacts, please use the following repository. The HuggingFace model downloader provides the following functionalities:</p> <p>1) Support for concurrent files download  2) Resumable download  3) Filter files download based on patterns and serialization format (e.g., .bin, .pt, .safetensors)  4) Package model in tarball format for further usage (e.g., vLLM) </p> <p>For example to download tiiuae/falcon-7b-instruct you would use this command: <pre><code>poetry run python3 src/hf_downloader.py --model-name \"tiiuae/falcon-7b-instruct\" \\\n                                        --revision \"cf4b3c42ce2fdfe24f753f0f0d179202fea59c99\" \\\n                                        --max-workers 8 \\\n                                        --save-as-tarfile \\\n                                        --tarfile-name \"tiiuae--falcon-7b-instruct\"\n</code></pre> For full options regarding the HuggingFace downloader CLI, refer to this repository.</p> <p>The model artefacts will be saved as a tarfile format that will be later transffered to our S3 bucket and be used in the inference process.</p> <p>The content of the tarball file should follow this directory structure:</p> <pre><code>tiiuae--falcon-7b-instruct\n\u251c\u2500\u2500 models--tiiuae--falcon-7b-instruct\n\u2502   \u251c\u2500\u2500 blobs\n\u2502   \u2502   \u251c\u2500\u2500 02b145e38790e52c2161b8d5ed97ee967bc3307e\n\u2502   \u2502   \u251c\u2500\u2500 1d92decce70fb4d5e840694c6947d2abfef27fdf\n\u2502   \u2502   \u251c\u2500\u2500 1de823c84b1c8b9889ac2a6c670ec6002a71776abd42cdf51bb3acd4c9938b29\n\u2502   \u2502   \u251c\u2500\u2500 24f2d2e20d26ae4f0729da0d008e0ee6d81fc560\n\u2502   \u2502   \u251c\u2500\u2500 24f43d813328da380b3d684c019f9c6d84df6b50\n\u2502   \u2502   \u251c\u2500\u2500 4aa644a0eca5b539ec8703d62d4b957c74a54963\n\u2502   \u2502   \u251c\u2500\u2500 66acf4bebb68593952a51575cb02dbf258a606e236c6b82b6b60c3b1e9089e66\n\u2502   \u2502   \u251c\u2500\u2500 70c059b13f10f3e20d34a31174dd7da53cfb93ad\n\u2502   \u2502   \u251c\u2500\u2500 834822cbe81585262b727c3fdbe520a34fd24ad4\n\u2502   \u2502   \u251c\u2500\u2500 84d8843072cbc300692c6bccff5b9c08c430498e\n\u2502   \u2502   \u2514\u2500\u2500 def8c2bf7e0f85be115be9e6a79dd3c5aa50a99d\n\u2502   \u251c\u2500\u2500 refs\n\u2502   \u2502   \u2514\u2500\u2500 main\n\u2502   \u2514\u2500\u2500 snapshots\n\u2502       \u2514\u2500\u2500 cf4b3c42ce2fdfe24f753f0f0d179202fea59c99\n\u2502           \u251c\u2500\u2500 config.json -&gt; ../../blobs/84d8843072cbc300692c6bccff5b9c08c430498e\n\u2502           \u251c\u2500\u2500 configuration_falcon.py -&gt; ../../blobs/def8c2bf7e0f85be115be9e6a79dd3c5aa50a99d\n\u2502           \u251c\u2500\u2500 generation_config.json -&gt; ../../blobs/02b145e38790e52c2161b8d5ed97ee967bc3307e\n\u2502           \u251c\u2500\u2500 handler.py -&gt; ../../blobs/70c059b13f10f3e20d34a31174dd7da53cfb93ad\n\u2502           \u251c\u2500\u2500 modeling_falcon.py -&gt; ../../blobs/834822cbe81585262b727c3fdbe520a34fd24ad4\n\u2502           \u251c\u2500\u2500 pytorch_model-00001-of-00002.bin -&gt; ../../blobs/66acf4bebb68593952a51575cb02dbf258a606e236c6b82b6b60c3b1e9089e66\n\u2502           \u251c\u2500\u2500 pytorch_model-00002-of-00002.bin -&gt; ../../blobs/1de823c84b1c8b9889ac2a6c670ec6002a71776abd42cdf51bb3acd4c9938b29\n\u2502           \u251c\u2500\u2500 pytorch_model.bin.index.json -&gt; ../../blobs/1d92decce70fb4d5e840694c6947d2abfef27fdf\n\u2502           \u251c\u2500\u2500 special_tokens_map.json -&gt; ../../blobs/24f43d813328da380b3d684c019f9c6d84df6b50\n\u2502           \u251c\u2500\u2500 tokenizer.json -&gt; ../../blobs/24f2d2e20d26ae4f0729da0d008e0ee6d81fc560\n\u2502           \u2514\u2500\u2500 tokenizer_config.json -&gt; ../../blobs/4aa644a0eca5b539ec8703d62d4b957c74a54963\n\u2514\u2500\u2500 version.txt\n</code></pre> <p>2.Saving model artefacts to S3 object store.</p> <p>In the IES GenAI Platform deployment workflow, we do not depend on the vLLM to download models from HuggingFace, as we utilised our own downloader in the previous section.</p> <p>To copy the locally downloaded model artefacts tarfile to our AWS S3 bucket, use the following command:</p> <p><pre><code>aws s3 cp &lt;tarfileName&gt; s3://&lt;bucket-id&gt;/models/&lt;modelFamily&gt;/&lt;revision&gt;/&lt;tarfileName&gt;\n# eg. aws s3 cp tiiuae--falcon-7b.tar.gz s3://&lt;bucket-id&gt;/models/falcon/cf4b3c4/tiiuae--falcon-7b-instruct.tar.gz\n</code></pre> For standardisation purposes, please adhere to the following path prefix:</p> <p><pre><code>s3://bucket/models/&lt;modelFamily&gt;/&lt;revision&gt;\n</code></pre> For revision, if the revision is cf4b3c42ce2fdfe24f753f0f0d179202fea59c99, you may just use the first 7 characters: cf4b3c4. </p>"},{"location":"llm-deployments/chat-models/deployment/#creating-llm-servingtemplate-for-ai-core-deployment","title":"Creating LLM ServingTemplate for AI Core Deployment","text":"<p>All our existing self-hosted LLM ServingTemplates can be found under the following repository. For this documentation, you can find the ServingTemplate we are using to deploy the falcon-7b-instruct model here.</p> <p>As the ServingTemplate may be complicated with various parameters to change, below provides the shortened version that highlights the important changes to make:</p> <pre><code>apiVersion: ai.sap.com/v1alpha1\nkind: ServingTemplate\nmetadata:\n  name: falcon-7b-instruct-cf4b3c4     # this is the servingtemplate name\n  annotations:\n    scenarios.ai.sap.com/description: \"SAP Hosted models on SAP AI Core\"\n    scenarios.ai.sap.com/name: \"sap-hosted-model\"\n    executables.ai.sap.com/description: \"AI Core executable for Open-source LLMs\"\n    executables.ai.sap.com/name: \"aicore-opensource\"\n  labels:\n    scenarios.ai.sap.com/id: \"sap-hosted-model\"       # scenarioID for servingTemplate\n    executables.ai.sap.com/id: \"aicore-opensource\"    # executableID for servingTemplate\n    ai.sap.com/version: \"1.0.0\"\nspec:\n  inputs:\n    parameters:\n      - name: image                                   # vLLM docker image, use the latest standardised one on our image repository\n        type: \"string\"\n        default: \"ies-aicoe.common.repositories.cloud.sap/genai-platform-exp/aicore-vllm:v090124-01\"\n      - name: resourcePlan                            # resourceplan\n        type: \"string\"\n        default: \"infer2.4xl\"\n              ...\n      - name: modelId                                 # HuggingFace model ID\n        type: \"string\"\n        default: \"tiiuae/falcon-7b-instruct\"\n      - name: revision\n        type: \"string\"\n        default: \"cf4b3c42ce2fdfe24f753f0f0d179202fea59c99\"\n      - name: modelName                               # vLLM application model name\n        type: \"string\"\n        default: \"tiiuae--falcon-7b-instruct\"\n    artifacts:\n    - name: tiiuae_falcon_7b_instruct                 # artifact for object store (MUST be alphanumeric and underscore)\n  template:\n    apiVersion: \"serving.kserve.io/v1beta1\"\n    metadata:\n            ...\n    spec: |\n      predictor:\n                ...\n        containers:\n                    ...\n          - name: STORAGE_URI\n            value: \"{{inputs.artifacts.tiiuae_falcon_7b_instruct}}\"   # change the following name to the `artifacts` values\n                        ...\n          volumeMounts:\n                        ...\n        volumes:\n                    ...\n</code></pre>"},{"location":"llm-deployments/chat-models/deployment/#deployment-settings-on-ai-launchpad","title":"Deployment settings on AI Launchpad","text":""},{"location":"llm-deployments/chat-models/deployment/#3-creating-application-to-sync-servingtemplate-sap-ai-core-administration","title":"3. Creating Application to Sync ServingTemplate (SAP AI Core Administration)","text":""},{"location":"llm-deployments/chat-models/deployment/#4-creating-object-store-secret-on-ai-launchpad-ml-operations","title":"4. Creating Object Store Secret on AI Launchpad (ML Operations)","text":""},{"location":"llm-deployments/chat-models/deployment/#5-creating-models-on-ai-launchpad-ml-operations","title":"5. Creating Models on AI Launchpad (ML Operations)","text":""},{"location":"llm-deployments/chat-models/deployment/#6-creating-configurations-for-deployment-ml-operations","title":"6. Creating Configurations for Deployment (ML Operations)","text":""},{"location":"llm-deployments/chat-models/deployment/#7-creating-deployment-for-llm-ml-operations","title":"7. Creating Deployment for LLM (ML Operations)","text":""},{"location":"llm-deployments/chat-models/deployment/#what-are-the-pros-cons-vlmm","title":"What are the pros &amp; cons vLMM?","text":"<ul> <li> <p>seems very fast</p> </li> <li> <p>seems very easy to use</p> </li> <li> <p>Really new framework: small community, maybe not mature (unknown problems), may not be the dominant framework in the future and then not be maintained.</p> </li> <li> <p>have to write the connector yourself for less common models</p> </li> </ul>"},{"location":"llm-deployments/chat-models/deployment/#source-code","title":"Source Code","text":"<p>Compelte source code Source Code {:target=\"_blank\"}.</p>"},{"location":"llm-deployments/embed-models/deployment/","title":"Deploying Embedding Models on the AI Core with <code>infinity</code>","text":"<p>The steps shown here will surely change in the future and the whole thing will get easier. This blog post is for early adopters and experts with a certain ability to suffer.</p> <p><code>infinity</code> is a fast and easy to use library for embedding model inference and serving with a permissive license. It comes with seamless integration with popular Huggingface <code>transformers</code> models.</p>"},{"location":"llm-deployments/embed-models/deployment/#checklist","title":"Checklist","text":"<ul> <li>[ ] Your model is compatible with SentenceTransformers</li> <li>[ ] Access to an AI Core instance with access to the required GPU resource plans</li> </ul>"},{"location":"llm-deployments/embed-models/deployment/#model-checkpoint","title":"Model Checkpoint","text":"<p><code>infinity</code> supports a variety of generative transformer models in Huggingface <code>transformers</code>.</p> <p>A first simple check to see if your model checkpoint can be used with <code>infinity</code> is if it can be loaded using SentenceTransformer:</p> <pre><code>from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"&lt;your model&gt;\")\nsentences = [\n    \"This framework generates embeddings for each input sentence\",\n    \"Sentences are passed as a list of string.\",\n    \"The quick brown fox jumps over the lazy dog.\",\n]\nsentence_embeddings = model.encode(sentences)\n</code></pre>"},{"location":"llm-deployments/embed-models/deployment/#preparing-and-testing-the-deployment-code","title":"Preparing and Testing the Deployment Code","text":"<p>The next step is to interactively test the model checkpoint in infinity on a GPU machine.</p> <p><pre><code>$ pip install infinity-emb[all]\n</code></pre> Going back to the example from earlier , running the mdoel would look like this:</p> <p><pre><code>import asyncio\nfrom infinity_emb import AsyncEmbeddingEngine, EngineArgs\n\nsentences = [\"Embed this is sentence via Infinity.\", \"Paris is in France.\"]\nengine = AsyncEmbeddingEngine.from_args(EngineArgs(model_name_or_path = \"&lt;your-model&gt;\", engine=\"torch\"))\n\nasync def main():\n    async with engine: # engine starts with engine.astart()\n        embeddings, usage = await engine.embed(sentences=sentences)\n    # engine stops with engine.astop()\nasyncio.run(main())\n    print(f\"Prompt: {prompt!r}, Generated text: {generated_text!r}\")\n</code></pre> Alternatively, we can start a server locally with the following command: <pre><code>infinity_emb --model-name-or-path \"&lt;your-model&gt;\"\n</code></pre> A test request can be sent like this: <pre><code>curl http://localhost:8000/embeddings \\\n    -H \"Content-Type: application/json\" \\\n    -d '{\n        \"model\": \"&lt;your-model&gt;\",\n        \"input\": [\"A sentence to encode.\"],\n    }'\n</code></pre> This is the type of server we will use on AI Core. The API provided by the server is compatible with the OpenAI API.</p>"},{"location":"llm-deployments/embed-models/deployment/#prepare-ai-core-deployment","title":"Prepare AI Core Deployment","text":"<p>After validating that the model checkpoint can be used for a <code>infinity</code> server the next step is to start this server in an AI Core deployment. This requires three things: 1. A deployment template in a git repository onboarded to AI Core 2. A model checkpoint in an object store onboarded to AI Core 3. A docker image in a repository onboarded to AI Core</p> <p>There are excellent resources on how to create an AI Core instance and how to complete the onboarding and thus we won't cover this here in this post.</p>"},{"location":"llm-deployments/embed-models/deployment/#deployment-template","title":"Deployment Template","text":"<p>The following deployment template can be used and exposes the most important deployment options as parameters.</p> <p><pre><code>apiVersion: ai.sap.com/v1alpha1\nkind: ServingTemplate\nmetadata:\n  name: nomic-embed ## This is the ServingTemplate's Name\n  annotations:\n    scenarios.ai.sap.com/description: \"CPIT Hosted models on SAP AI Core\" ## This is the scenario Name\n    scenarios.ai.sap.com/name: \"cpit-foundation-models\"\n    executables.ai.sap.com/description: \"AI Core executable for CPIT Open-source LLMs\"\n    executables.ai.sap.com/name: \"nomic-embed\" ## This is the executables Name\n  labels:\n    scenarios.ai.sap.com/id: \"ies-foundation-models\"\n    executables.ai.sap.com/id: \"nomic-embed\" # Adjust for your model\n    ai.sap.com/version: \"1.0.0\"\n    scenarios.ai.sap.com/llm: \"true\"\nspec:\n  inputs:\n    parameters:\n      - name: image\n        type: \"string\"\n        default: \"XXX.common.repositories.cloud.sap/genai-platform-exp/ai-core-embed-model-serve:0.0.1\" # This is the image used in JFrog, if you have not rebuilt this, it needs no adjustment.\n      - name: resourcePlan\n        type: \"string\"\n        default: \"infer2.4xl\" # If the model does not require any specific hardware you can leave this see wiki page on vLLM.\n      - name: tokenizer\n        type: \"string\"\n        default: \"\"\n      - name: minReplicas\n        type: \"string\"\n        default: \"1\"\n      - name: maxReplicas\n        type: \"string\"\n        default: \"1\"\n      - name: portNumber\n        type: \"string\"\n        default: \"9000\"\n      - name: gpu\n        type: \"string\"\n        default: \"4\"\n      - name: trustRemoteCode\n        type: \"string\"\n        default: \"true\"\n      - name: batchSize\n        type: \"string\"\n        default: \"128\"\n      - name: urlPrefix\n        type: \"string\"\n        default: \"/v1\"\n      - name: additionalArgument\n        type: \"string\"\n        default: \"\"\n      - name: modelName\n        type: \"string\"\n        default: \"nomic-ai/nomic-embed-text-v1.5\" # This is the model name which later has to be used in the API call.\n    artifacts:\n    - name: nomicv15 # This is the name of your artifact, best to avoid any special characters like \"-\" or \"_\". You need to change this in the specs STORAGE_URI below.\n  template:\n    apiVersion: \"serving.kserve.io/v1beta1\"\n    metadata:\n      annotations: |\n        autoscaling.knative.dev/metric: concurrency\n        autoscaling.knative.dev/target: 1\n        autoscaling.knative.dev/targetBurstCapacity: -1\n        autoscaling.knative.dev/window: \"10m\"\n        autoscaling.knative.dev/scaleToZeroPodRetentionPeriod: \"10m\"\n      labels: |\n        ai.sap.com/resourcePlan: \"{{inputs.parameters.resourcePlan}}\"\n    spec: |\n      predictor:\n        imagePullSecrets:\n        - name: dab-genai-platform-artifactory\n        minReplicas: {{inputs.parameters.minReplicas}}\n        maxReplicas: {{inputs.parameters.maxReplicas}}\n        containers:\n        - name: kserve-container\n          image: \"{{inputs.parameters.image}}\"\n          ports:\n          - containerPort: {{inputs.parameters.portNumber}}\n            protocol: TCP\n          env:\n          - name: STORAGE_URI\n            value: \"{{inputs.artifacts.nomicv15}}\"\n          - name: TRUST_REMOTE_CODE\n            value: \"{{inputs.parameters.trustRemoteCode}}\"\n          - name: BATCH_SIZE\n            value: \"{{inputs.parameters.batchSize}}\"\n          - name: URL_PRE_FIX\n            value: \"{{inputs.parameters.urlPrefix}}\"\n          - name: MODEL_SERVE_NAME\n            value: \"{{inputs.parameters.modelName}}\"\n          - name: ARG\n            value: \"{{inputs.parameters.additionalArgument}}\"\n          - name: HUGGINGFACE_HUB_CACHE\n            value: \"/mnt/models\" # This is the path where the model will be stored in the container by default. Change this to '/nonexistent/models' for Huggingface Model Download\n          volumeMounts:\n          - name: shm\n            mountPath: /dev/shm\n        volumes:\n        - name: shm\n          emptyDir:\n            medium: Memory\n            sizeLimit: 10Gi\n</code></pre> This workflow template is included in the repository (<code>serve-emb-infinity.yaml</code>).</p>"},{"location":"llm-deployments/embed-models/deployment/#model-weight-download","title":"Model weight download","text":"<p>download model weights. For example via: <pre><code>mkdir nomic-embed-text-v1.5 &amp;&amp; cd nomic-embed-text-v1.5\ngit lfs install\ngit clone https://huggingface.co/nomic-ai/nomic-embed-text-v1.5\n</code></pre></p>"},{"location":"llm-deployments/embed-models/deployment/#model-checkpoint_1","title":"Model Checkpoint","text":"<p>Transfer your model checkpoint to an object store. For example via: <pre><code>aws s3 cp --recursive &lt;path-to-your-checkpoint-folder&gt; s3://on-boarded-bucket/our-awesome-model/v1\n</code></pre></p> <p>Next we create an artifact for the checkpoint. Set <code>url</code> to <code>ai://{object-secret-name}/{path-rel-to-pathPrefix-of-the-secret}</code> and <code>scenarioId</code> to the value used in the deployment template:</p> <p><pre><code>{\n  \"labels\": [],\n  \"name\": \"our-awesome-model/v1\",\n  \"kind\": \"model\",\n  \"url\": \"ai://s3-bucket/our-awesome-model/v1\",\n  \"description\": \"Fantastic model\",\n  \"scenarioId\": \"&lt;your-scenario&gt;\"\n}\n</code></pre> The response to the <code>POST</code>-request contains an <code>artifactid</code> e.g. <code>aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa</code>.</p>"},{"location":"llm-deployments/embed-models/deployment/#docker-image","title":"Docker Image","text":"<p>The final missing piece is the docker image.</p> <p><pre><code>FROM nvcr.io/nvidia/pytorch:24.01-py3 AS runtime\n\nWORKDIR /usr/src\n\nRUN mkdir /nonexistent &amp;&amp; \\\n    chmod 777 /nonexistent\n\nRUN python3 -m pip install --upgrade pip==23.2.1 &amp;&amp; \\\n    python3 -m pip install \"infinity-emb[all]\" &amp;&amp; \\\n    rm -rf /root/.cache/pip\n\n# These are environment variable from Huggingface\n# https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables\nENV HUGGINGFACE_HUB_CACHE=\"/tmp\" \\\n    HUGGING_FACE_HUB_TOKEN=\"\" \\\n    HF_HOME=\"/nonexistent\" \\\n    HF_HUB_OFFLINE=\"0\" \\\n    HF_HUB_DISABLE_TELEMETRY=\"1\" \\\n    SERVE_FILES_PATH=\"/mnt/models\"\n\nCOPY run.sh /usr/src/run.sh\nRUN chmod +x /usr/src/run.sh\n\nCMD [ \"/usr/src/run.sh\" ]\n</code></pre> This Dockerfile and the <code>run.sh</code>-file are part of this repo and both can be usable without modification. The <code>run.sh</code> file starts the <code>infinity</code> server using the template parameters. Once the docker image is built push it to the docker repository registered in AI Core under the name used as the replacement for <code>&lt;your-docker-repo-secret&gt;</code> in the workflow template. In the next step we will assume the image was push as <code>XXX.common.repositories.cloud.sap/ai-core-infinity-model-serve:0.0.1</code>.</p>"},{"location":"llm-deployments/embed-models/deployment/#start-deployment","title":"Start Deployment","text":"<p>To start the deployment we have to send a <code>POST</code>-request to <code>{apiurl}/v2/lm/configurations</code> to create a configuration:</p> <p><pre><code>        {\n            \"executableId\": \"infinity-emb-model-serve\",\n            \"inputArtifactBindings\": [\n                {\n                    \"artifactId\": \"aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaaa\",\n                    \"key\": \"embmodel\"\n                }\n            ],\n            \"name\": \"nomic-embed-text-v1.5\",\n            \"parameterBindings\": [\n                {\n                    \"key\": \"image\",\n                    \"value\": \"XXX.common.repositories.cloud.sap/genai-platform-exp/ai-core-embed-model-serve:0.0.1\"\n                },\n                {\n                    \"key\": \"batchSize\",\n                    \"value\": \"256\"\n                },\n                {\n                    \"key\": \"trustRemoteCode\",\n                    \"value\": \"False\"\n                },\n                {\n                    \"key\": \"portNumber\",\n                    \"value\": \"9000\"\n                },\n                {\n                    \"key\": \"resourcePlan\",\n                    \"value\": \"infer2.l\"\n                },\n                {\n                    \"key\": \"modelName\",\n                    \"value\": \"nomic-ai/nomic-embed-text-v1.5\"\n                }\n            ],\n            \"scenarioId\": \"llm-fine-tuning\"\n        }\n</code></pre> The response to the request will contain a <code>configurationid</code>, e.g. <code>cccccccc-cccc-cccc-cccc-cccccccccccc</code>.</p> <p>With this ID we can start the deployment by sending a <code>POST</code> request to <code>{apiurl}/v2/lm/configurations/cccccccc-cccc-cccc-cccc-cccccccccccc/deployments</code>.</p>"},{"location":"llm-deployments/embed-models/deployment/#use-deployment-via-gen-ai-hub-sdk","title":"Use Deployment via <code>gen-ai-hub-sdk</code>","text":"<p>The deployment we created can be used in Python via <code>generative-ai-hub-sdk</code>. Make sure that <code>generative-ai-hub-sdk</code> is configured to use AI Core proxies.</p> <p>The previous steps deploy an OpenAI-style server, so the deployment can be used via the OpenAI classes from the package. The only additional step is to tell the package which scenario and which configurations are embedding model deployments.</p> <p><pre><code>from gen_ai_hub.proxy.native.openai import OpenAI\nfrom gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n\nGenAIHubProxyClient.add_foundation_model_scenario(\n    scenario_id='&lt;your-scenario&gt;',\n    config_names=[\"nomic-embed-text-*\"],\n    prediction_url_suffix='v1/embeddings'\n)\n</code></pre> This function instructs the client to treat all <code>RUNNING</code> deployments in the <code>&lt;your-scenario&gt;</code> scenario with a configuration name matching <code>nomic-embed-text-*</code> as a foundational model deployments and that the prediction url is the deployment url with <code>/v1/embeddings</code> as a suffix.</p> <p>After running this function the models can be used in the same way as a centrally hosted model:</p> <pre><code>proxy_client = GenAIHubProxyClient()\nopenai = OpenAI(proxy_client=proxy_client)\n\ndef get_detailed_instruct(task_description, query):\n    return f'Instruct: {task_description}\\nQuery: {query}'\n\n# For \"nomic-ai/nomic-embed-text-v1.5\" each query must come with a one-sentence instruction that describes the task\ntask = 'Given a web search query, retrieve relevant passages that answer the query'\nqueries = [\n    get_detailed_instruct(task, 'how much protein should a female eat'),\n]\nemb = openai.embeddings.create(model='nomic-ai/nomic-embed-text-v1.5', input=queries)\n</code></pre> <p>This post is designed to give you a head start on setting up your own embedding models on AI Core. When deploying models, there are many technical details that play a role and must be taken into account. Expect that the best way to deploy an embedding will change on a regular basis. Also, don't forget about the costs. You'll need to keep those models under high utilization if you want to keep your spending anywhere near what the big commercial companies charge.</p>"},{"location":"llm-deployments/embed-models/deployment/#source-code","title":"Source code","text":"<p>Complete source code example Source Code {:target=\"_blank\"}.</p>"},{"location":"llm-deployments/fine-tunemodels/deployment/","title":"Fine-tuned models: vLLM framework","text":""},{"location":"llm-deployments/fine-tunemodels/deployment/#about-vllm","title":"About vLLM","text":"<p>About vLLM</p>"},{"location":"llm-deployments/fine-tunemodels/deployment/#how-to-get-started-with-vllm-and-local-deployment","title":"How to get started with vLLM and Local Deployment","text":"<p>Local deployment</p>"},{"location":"llm-deployments/fine-tunemodels/deployment/#retrieving-packaging-fine-tuned-model-weights-and-saving-to-s3-object-store","title":"Retrieving, Packaging Fine-tuned Model Weights and Saving to S3 Object Store","text":"<p><pre><code>The key idea for the fine-tuned model weights packaging is that: 1) it must be HuggingFace Hub format with specific required files.\n</code></pre> For our exploration in hosting the fine-tuned models, we downloaded model weights that were fine-tuned via ADB but utilised models that are downloaded from HuggingFace. For example, we can take a look at the following model artifacts available post fine-tuning:  </p> <p>On the left, we have the model artifacts setup from ADB post fine-tuning, while on the right, we have the original model weight setup hosted on HuggingFace Hub. As you can see, the following files should always be present to ensure that inference via vLLM can work as expected:</p> <p>1.config.json  2.generation_config.json  3.weights (.safetensors, .bin format)  4.model.{weight_format).index.json  5.special_tokens_map.json  6.tokenizer.json  7.tokenizer.model  8.tokenizer_config.json </p> <p><pre><code>If you have used model downloaded from HuggingFace Hub for model fine-tuning, rest assured the format post fine-tuning should usually work.\n</code></pre> Once you retrieved all the relevant files, you will put them in a single folder like the following:</p> <pre><code>.\n\u2514\u2500\u2500 main\n    \u251c\u2500\u2500 config.json\n    \u251c\u2500\u2500 generation_config.json\n    \u251c\u2500\u2500 model-00001-of-00003.safetensors\n    \u251c\u2500\u2500 model-00002-of-00003.safetensors\n    \u251c\u2500\u2500 model-00003-of-00003.safetensors\n    \u251c\u2500\u2500 model.safetensors.index.json\n    \u251c\u2500\u2500 special_tokens_map.json\n    \u251c\u2500\u2500 tokenizer.json\n    \u251c\u2500\u2500 tokenizer.model\n    \u2514\u2500\u2500 tokenizer_config.json\n</code></pre> <p>For instance, if you were to leave the model weights set up like the above, when you compress the weights folder into a tarfile as indicated in Chat  models: vLLM framework, your path to the weights should be: /main .</p> <p>As another example, if you will to put the model weights in the following structure: <pre><code>.\n\u251c\u2500\u2500 snapshots\n\u2502   \u2514\u2500\u2500 main\n\u2502       \u251c\u2500\u2500 config.json\n\u2502       \u251c\u2500\u2500 generation_config.json\n\u2502       \u251c\u2500\u2500 model-00001-of-00003.safetensors\n\u2502       \u251c\u2500\u2500 model-00002-of-00003.safetensors\n\u2502       \u251c\u2500\u2500 model-00003-of-00003.safetensors\n\u2502       \u251c\u2500\u2500 model.safetensors.index.json\n\u2502       \u251c\u2500\u2500 special_tokens_map.json\n\u2502       \u251c\u2500\u2500 tokenizer.json\n\u2502       \u251c\u2500\u2500 tokenizer.model\n\u2502       \u2514\u2500\u2500 tokenizer_config.json\n</code></pre> Your new path to the weights should be: /snapshots/main Once you have figured your preferred way of storing the weights, you can simply compress the model weights folder as below:</p> <pre><code># tar -cf {tarfile-name}.tar.gz {model-weights-folder-name}\n\ntar -cf mistralai-Mistral-7B-Instruct-v0_2-ft.tar.gz models--mistralai--Mistral-7B-Instruct-v0_2\n\n# or\n\ntar -cf model-weights.tar.gz snapshots      # based on the above examples\n\n# or\n\ntar -cf model-weights.tar.gz main           # based on the above examples\n</code></pre>"},{"location":"llm-deployments/fine-tunemodels/deployment/#how-to-get-started-with-vllm-and-docker-deployment","title":"How to get started with vLLM and Docker Deployment","text":""},{"location":"llm-deployments/fine-tunemodels/deployment/#building-vllm-inference-server-docker-image","title":"Building vLLM Inference Server Docker Image","text":"<p>As the vLLM Inference Server Docker image is fundamentally a Docker image that only contains the vLLM setup, individual developers would NOT be required to build a new vLLM image every time a new deployment is conducted. As such, a standardised repository path is created to host the [latest] image that can be re-used across different deployments.</p> <pre><code># From dir where you want to clone the repository:\ngit clone &lt;https://github.tools.sap/Artificial-Intelligence-CoE/AICore-vLLM.git&gt;\n\n# From the AICore-LLM directory\ncd inference-server\n\n# Build docker image\ndocker build -t &lt;dockerRepository&gt;/aicore-vllm:v&lt;ddmmyy&gt;-&lt;minorVersion&gt; .\n# e.g.,\n\n# Push docker image to the registered repository\ndocker push &lt;dockerRepository&gt;/aicore-vllm:v&lt;ddmmyy&gt;-&lt;minorVersion&gt;\n# e.g.,\n</code></pre>"},{"location":"llm-deployments/fine-tunemodels/deployment/#run-vllm-inference-via-docker-image","title":"Run vLLM Inference via Docker Image","text":""},{"location":"llm-deployments/fine-tunemodels/deployment/#aicore-deployment","title":"AICore deployment","text":"<p>AICore deployment</p>"},{"location":"llm-deployments/fine-tunemodels/deployment/#source-code","title":"Source Code","text":"<p>Compelte source code Source Code {:target=\"_blank\"}.</p>"},{"location":"proxy-deployments/deployment/","title":"Proxy deployments: LiteLLM framework","text":"","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#introduction","title":"Introduction","text":"","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#remote-hosted-models","title":"Remote Hosted Models:","text":"<p>In this approach, the LLM model is hosted on external servers or cloud infrastructure, allowing users to access and interact with the model through APIs over the internet. This is often used for applications where scalability and accessibility are essential.</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#litellm","title":"LiteLLM:","text":"<p>LiteLLM gives us the ability to handle requests for multiple LLM models that are hosted remotely through a single interface. To use LiteLLM for remote hosted models, you can implement proxy server that supports LiteLLM. More information on LiteLLM, benefits of using LiteLLM and our deployment strategy of the LiteLLM proxy on AI Core are listed in the next sections.</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#what-is-litellm","title":"What is LiteLLM ?","text":"<p>LiteLLM simplifies calling LLM providers by providing a consistent input/output format for calling all models using the OpenAI format. It\u2019s a middleware that acts as an intermediary between the client application and the language model API services such as Azure, Anthropic, OpenAI, and others. The primary purpose of LiteLLM Proxy is to streamline and simplify the process of making API calls to these services by providing a unified interface. Making it easy for you to add new models to your system in minutes (using the same exception-handling, token logic, etc. you already wrote for OpenAI).</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#why-litellm","title":"Why LiteLLM ?","text":"<p>Calling multiple LLM providers involves messy code - each provider has it\u2019s own package and different input/output. Langchain is too bloated and doesn\u2019t provide consistent I/O across all LLM APIs. When we added support for multiple llms on our application. APIs can fail (e.g. Azure readtimeout errors), so we wrote a fallback strategy to iterate through a list of models in case one failed (e.g. if Azure fails, try Cohere first, OpenAI second etc.). Provider-specific implementations meant our for-loops became increasingly large (think: multiple ~100 line if/else statements), and since made LLM API calls in multiple places in our code, our debugging problems exploded. Because now we had multiple for-loop chunks across our codebase.</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#who-can-benefit-litellm","title":"Who can benefit LiteLLM ?","text":"<p>Abstraction. That\u2019s when LiteLLm decided to abstract our api calls behind a single class. We needed I/O that just worked, so we could spend time improving other parts of our system (error-handling/model-fallback logic, etc.). This class needed to do 3 things really well:</p> <p>Consistent I/O Format: LiteLLM Proxy uses the OpenAI format for all models. This means that regardless of the LLM model you are interacting with, the format for sending requests and receiving responses remains consistent. Handling requests for multiple LLM Models: The ability to handle requests for multiple LLM models. It can make /chat/completions requests for more than 50 LLM models, including Azure, OpenAI, Replicate, Anthropic, and Hugging Face. Model fallbacks: Error handling is a crucial aspect of any application, and LiteLLM Proxy excels in this regard. It uses model fallbacks for error handling. If a model fails, it tries another model as a fallback.How to learn More about LiteLLM? Please refer LiteLLM Document page for more information.</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#poc-deploy-litellm-proxy","title":"PoC: Deploy LiteLLM Proxy","text":"<p>We are doing LiteLLM POC  for Explore on how to connect multiple LLM Providers, Deploy LiteLLM in Docker &amp; AICore as proxy server and provide proxy server inference in OpenAI Format.</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#local-development","title":"Local Development","text":"","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#pre-requisite","title":"Pre-requisite","text":"<ul> <li>Python Installation</li> <li>GitCLi Installation</li> <li>Docker Installation</li> </ul>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#clone-litellm-from-litellm","title":"Clone liteLLM from Litellm","text":"<p>Clone liteLLM server code  from Litellm Repo <pre><code>git clone https://github.com/BerriAI/litellm.git\n</code></pre> Note: Update the repo to support proxy deployment for finetuned models  if model serve support openai format lile vLLM library</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#prepare-docker-image","title":"Prepare Docker Image","text":"<p>Default behaviour LiteLLM docker file is when you build Docker image it  will run automatically so we need to change the default behaviour to run docker when we use the run command - Navigate to Docker file - Comment the RUN command in docker file - Add CMD commend to run docker file</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#build-docker-image","title":"Build Docker Image","text":"<p>Open VS Integrated terminal from VS Studio code or Open command line navigate to project directory and Run below command to build a docker file</p> <p><pre><code>docker build  --platform linux/amd64 -t genai-platform-exp/litellm-proxy-poc:01 .\n</code></pre> Now, verify if the image is built successfully.</p> <p>To see the list of docker images, run the below command:</p> <pre><code>Docker images\n</code></pre>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#run-docker-image","title":"Run Docker Image","text":"<p> Note:  Test docker image locally please add llm keys to secrets_template.toml or .env.</p> <p><pre><code>docker run -p 8000:8000 -d genai-platform-exp/litellm-proxy-poc:01\n</code></pre> Note: Test docker image locally please add llm keys to secrets_template.toml.</p> <p>Once run successfully we can see the running image id and also we can see local binding instance.</p> <p>For swagger documentation (http://localhost:8000/)</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#push-docker-image","title":"Push Docker Image","text":"<p>Before pushing to Docker repository, ensure you have logged in to the Docker Artifactory by following commands.</p> <p>docker login dockerhub provide your credentials for logging in (if logging in first time)</p> <p>Push the docker image by running following command.</p> <p><pre><code>docker push genai-platform-exp/litellm-proxy-poc:01\n</code></pre> - Navigate to project directory - Docker push command - Once successfully pushed you can find docker id Check whether the newly created docker image is present in the Docker hub.</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#aicore-deployment","title":"AICore deployment","text":"","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#pre-requisite_1","title":"Pre-requisite","text":"<p>Basic of AICore &amp; AILunch Pad</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#generic-secrets","title":"Generic Secrets","text":"<p>LiteLLM proxy-server always fetch llms authentication keys &amp; configurations from environment variables to connect multiple llms. As per security standards we can add environments variables directly at project level. To Solve above security problem we can use AICore Generic secret to save llm authentication information. AICore Generic secret  accept only encoded format so we can resolve security problem also.</p> <p>Create Generic secrets in AICore in two ways</p> <ul> <li>AI Launchpad ( URL )</li> <li>Postman Tool</li> </ul>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#serve-template","title":"Serve Template","text":"<p>Serve template used for deploy litellm proxy application in AICore. LiteLLM required environment variable to connect multiple remote llm servers. AICore standards we can defined environment variables at serving template level. you can bind generic secret in Serve template.</p> <ul> <li>Executable ID \u2013 unique identifier of the workflow template</li> <li>Scenario ID: Give the scenario id:</li> <li>Resource plan: Specify resource configuration for Application</li> <li>Docker Registry secret:  name Docker registry which is already configured in AICore</li> <li>Docker Image: Provided the created docker image which was later pushed into artifactory.</li> <li>Environment variable: Name of the environment variable, this variable  available at application environment level</li> <li>Generic Secret: Name of the Generic secret to read reference value</li> <li>Generic Secret Key: Key name of the generic secret , it pair with encoded environment variable.</li> </ul>","tags":["proxy-deployment","LiteLLm"]},{"location":"proxy-deployments/deployment/#supported-providers-models","title":"Supported Providers Models","text":"<p>Litellm support models</p>","tags":["proxy-deployment","LiteLLm"]},{"location":"tags/","title":"Tags","text":""},{"location":"tags/#litellm","title":"LiteLLm","text":"<ul> <li>LiteLLM</li> </ul>"},{"location":"tags/#proxy-deployment","title":"proxy-deployment","text":"<ul> <li>LiteLLM</li> </ul>"}]}